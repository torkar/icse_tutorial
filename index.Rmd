---
title: "Bayesian Data Analysis in Empirical Software Engineering"
author: "R. Torkar, C. A. Furia, and R. Feldt"
date: "Version: `r Sys.time()`"
output: 
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache = TRUE)

library(rethinking) # the sw for model specification (it then uses cmdstan)
library(foreign) # need to load funky data format
library(here) # make sure working dir is the same all the time
library(dagitty) # drawing DAGs
library(ggdag) # also a DAG library (all DAG stuff commented out for now)
library(latex2exp)
set.seed(100)
```

# The data and the problem

For this example we'll make use of a dataset found in the PROMISE repository donated by Prof.&nbsp;Martin Shepperd in 2005, and originally from J.&nbsp;M.&nbsp;Desharnais' master thesis.^[http://promise.site.uottawa.ca/SERepository/datasets/desharnais.arff] We would like to predict `Effort` (our outcome) for implementing a software artifact, given programming language used (our predictor).

```{r}
f <- read.arff("data/desharnais.arff")
 
# remove columns we don't need
f <- f[-c(1:5,7:11)]

# convert Language (factor) to numeric
f$Language <- as.numeric(f$Language)

str(f)
```

So, from the top, we have `Effort`, our outcome, and then `Language`, our predictor. `Effort` is in hours spent. `Language` we've converted to an integer $1,2,3$, indicating three different languages. In total we have `r nrow(f)` rows (or observations, if you will).

## Step 1: Likelihoods

What ontological and epistemological assumptions can we make concerning the underlying process that generated the outcome `Effort`?

From an ontological perspective, the outcome consists of positive integers, i.e., $\mathbb{N}^+$, so a count going from $0 \rightarrow \infty$. For this type of data the $\mathsf{Poisson}(\lambda)$ distribution is commonly used, where $\lambda$ is the parameter we want to estimate.^[For the $\mathsf{Poisson}(\lambda)$, the $\lambda$ estimates the mean *and* the variance of the distribution.]

From an epistemological perspective, and from an information theoretical point of view, we want to use a likelihood that allows the data to happen in the most ways, i.e., it doesn't constrain the story data wants to tell us. Given the above, the maximum entropy distribution is the $\mathsf{Poisson}(\lambda)$.

Let us now design a set of models $\mathbf{M}=\{\mathcal{M}_0,\mathcal{M}_1\}$ and see how well they compare concerning out of sample predictions. For the first model we'll use prior and posterior predictive checks (which we won't report for the second model).

## Step 2: Priors

We want a simple intercept only model (to estimate the grand mean $\alpha$). A common prior is $\mathsf{Normal}(0,10)$, but since we use a log link when using a $\mathsf{Poisson}$ likelihood, this can have very strange effects.

Without looking at the empirical data we can assume that we perhaps have a mean `Effort` in the thousands (not much of a project otherwise). The maximum `Effort` could very well end up in the millions (50--60 people working for ten years in a project), but let's assume it's not in the billions (70,000 people working for ten years in a project).

```{r}
# sample randomly from a log-normal()
max(rlnorm(1e4, 0, 4))
```

which seems OK. Let's have a look at a default prior,

```{r}
# sample randomly from a log-normal()
set.seed(1)
max(rlnorm(1e4, 0, 10))
```

which seems absurd. Such a project would need the earth's population ($8 \cdot 10 ^9$) to work on the project for almost 3,000 years, if we assume they spent 1,500 hours/year on the project: $3.5 \cdot 10^{16} / 8 \cdot 10^9 / 1.5 \cdot 10^3 =$ `r round(3.5e16 / 8e9 / 1.5e3, 0)`.

Let's go for a $\mathsf{Normal}(0,4)$ as a prior on the $\alpha$ parameter that we want to estimate. 

## Step 3: Calculating the posterior

Sample the model with empirical data. We sample using four independent chains, which then are used to indicate if we have reached a stationary posterior distribution.

```{r m0, message=FALSE, warning=FALSE, results='hide'}
m0 <- ulam(
  alist(
    Effort ~ poisson(lambda),
    log(lambda) <- alpha, # log link
    alpha ~ normal(0, 4)
  ), data = f, cores = 4, chains = 4, cmdstan = TRUE, log_lik = TRUE, iter = 5e3
)
```

Check diagnostics.

```{r}
precis(m0)
```

The diagnostics looks OK. $\widehat{R} < 1.01$ and the effective sample size (`n_eff`) is in the thousands, so all is well there. Let's also have a look at the trankplots for our estimated parameters.

```{r, echo=FALSE, warning=FALSE, message=FALSE, results='hide', fig.align='center'}
trankplot(m0)
```

This is how healthy trankplots should look like, i.e., four chains mixing well after the initial phase. 

The estimated $\alpha$ was $8.5$. But remember, we used a log link so we need to get it back on the outcome scale, i.e., $\mathrm{exp}(8.5)=$ `r round(exp(precis(m0)[,1]), 0)`. That's the overall mean of `Effort`, no matter language used. 

Conducting posterior predictive checks when we only estimate a grand mean is really not sane so we'll leave this for now. Let's focus on adding adding our predictor `Language`.

We'll add `Language` as a varying intercept, i.e., each of the three languages get their own intercept.

```{r m1, message=FALSE, warning=FALSE, results='hide'}
m1 <- ulam(
    alist(
        Effort ~ poisson(lambda),
        log(lambda) <- a_lang[Language], # each lang its own intercept
        a_lang[Language] ~ normal(0, 3)
    ), data = f, cores = 4, chains = 4, cmdstan = TRUE, log_lik = TRUE
)
```

We've conducted prior predictive checks and checked diagnostics for the above model. Now might be interesting to do a posterior predictive check, i.e., how well our model fits the data.

## Step 4: Sanity check of the posterior

```{r, message=FALSE, warning=FALSE, results='hide'}
postcheck(m1, window = 81)
```

The vertical axis is the outcome for each case in our dataset on the horizontal. The blue dots are the empirical data. The circles are the posterior mean. We see that the model has terribly fit in many cases (i.e., many of the blue dots are far away from the circles). Hmmm$\ldots$ our priors are sane but given this large variation a $\mathsf{Poisson}$ might not be the right likelihood after all$\ldots$

Remember, a $\mathsf{Poisson}$ assumes that the mean and the variance is approximately equal. Let's check if this is true,

```{r}
var(f$Effort)
mean(f$Effort)
```

This clearly indicates that the $\mathsf{Poisson}$ should not be used. We need to fall back on a mixture distribution (a mixture of $\mathsf{Gamma}$ and $\mathsf{Poisson}$) often called $\mathsf{Negative}$-$\mathsf{Binomial}$. Here we will be able to model the variance separately.

```{r, m2, message=FALSE, warning=FALSE, results='hide'}
m2 <- ulam(
    alist(
        Effort ~ dgampois(lambda, phi), # phi to model variance
        log(lambda) <- a_lang[Language],
        a_lang[Language] ~ normal(0, 3),
        phi ~ exponential(1) # prior on the variance component phi
    ), data = f, cores = 4, chains = 4, cmdstan = TRUE, log_lik = TRUE,
    control = list(adapt_delta=0.95)
)
```

Let's do a posterior check again,

```{r}
postcheck(m2, window=81)
```

The blue dots are the empirical data. The circles are the posterior mean, with 90% interval, and the '+' symbol mark the 90% predicted interval. In some cases we see clear outliers (e.g., Cases 43 and 81), which are outside the 90% predicted interval. This is not strange, after all, if we want a perfect model for our empirical data, why not simply use the data as-is? Well, the answer to that question is: We will inadvertently overfit, i.e., learn too much from the empirical data, which will lead to a model that will break down when facing new data.

## (Step 5: Model comparisons)

We now have a set of models $\mathbf{M}$, which we can compare using PSIS-LOO. The comparison will be *relative* and will not indicate if we've found an optimal, 'true', model. It will simply rank the models according to their relative out of sample prediction capabilities.

```{r loo, warning=FALSE, message=FALSE, fig.align='center'}
(loo_est <- compare(m0, m1, m2, func=LOO))
```

What we see here, not surprisingly, is that $\mathcal{M}_2$ is considered the best model. But how much better is that model compared to the model that comes on the second place, i.e., $\mathcal{M}_1$?

We can actually calculate a confidence interval for it if we want to. Using a $z$-score of 1.96 (i.e., 95%) we can use the relative difference in PSIS (dPSIS) and the difference in standard error (dSE) to calculate the interval,

```{r}
loo_est[2,3] + c(-1,1) * loo_est[2,4] * 1.96
```

which is clearly not crossing zero. In short, modeling the variance separately using the $\mathsf{Negative}$-$\mathsf{Binomial}$ paid off.

## Step 6: Compute stuff

If we plot our estimates we might visually appreciate the differences between the estimates better.

```{r}
plot(precis(m2, depth = 2, pars = "a_lang", prob = 0.95))
```

Circles indicate the estimated posterior mean, and the bars are 95% posterior intervals. 
We see that one language (Language 3) is clearly lower than the other languages. In short, using Language 3 means that a project uses less effort. Since Language 3 clearly does not touch any of the other languages bars one could claim that it's a significant difference.
