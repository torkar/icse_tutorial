---
title: "Bayesian Data Analysis in Empirical Software Engineering"
author: "R. Torkar, C. A. Furia, and R. Feldt"
date: "Version: `r Sys.time()`"
output: 
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache = TRUE)

library(rethinking) # the sw for model specification (it then uses cmdstan)
library(foreign) # need to load funky data format
# library(here) # make sure working dir is the same all the time
set.seed(100)
```

# The data and the problem

For this example we'll make use of a dataset found in the PROMISE repository donated by Prof.&nbsp;Martin Shepperd in 2005, and originally from J.&nbsp;M.&nbsp;Desharnais' master thesis.^[http://promise.site.uottawa.ca/SERepository/datasets/desharnais.arff] We would like to predict `Effort` (our outcome) for implementing a software artifact, given programming language used (our predictor).

Data, script, etc. are found at the GitHub repository.^[https://github.com/torkar/icse_tutorial]

```{r}
f <- read.arff("data/desharnais.arff")

# remove columns we don't need
f <- f[-c(1:5, 7:11)]

str(f)
```

So, from the top, we have `Effort`, our outcome, and then `Language`, our predictor. `Effort` is in hours spent. `Language` we've converted to an integer $1,2,3$, indicating three different languages. In total we have `r nrow(f)` rows (or observations, if you will).

## Step 1: Likelihoods

What ontological and epistemological assumptions can we make concerning the underlying process that generated the outcome `Effort`?

From an ontological perspective, the outcome consists of positive integers, i.e., $\mathbb{N}^+$, so a count going from $0 \rightarrow \infty$. For this type of data the $\mathsf{Poisson}(\lambda)$ distribution is commonly used, where $\lambda$ is the parameter we want to estimate.^[For the $\mathsf{Poisson}(\lambda)$, the $\lambda$ estimates the mean *and* the variance of the distribution.]

From an epistemological perspective, and from an information theoretical point of view, we want to use a likelihood that allows the data to happen in the most ways, i.e., it doesn't constrain the story data wants to tell us. Given the above, the maximum entropy distribution is the $\mathsf{Poisson}(\lambda)$.

Let us now design a set of models $\mathbf{M}=\{\mathcal{M}_0,\ldots,\mathcal{M}_n\}$ and see how well they compare concerning out of sample predictions. For the first model we'll use prior predictive checks (which we won't report for the second model), while for the second model we'll do posterior predictive checks.

## Step 2: Priors

We want a simple intercept only model (to estimate the grand mean $\alpha$). A common prior is $\mathsf{Normal}(0,10)$, but since we use a log link when using a $\mathsf{Poisson}$ likelihood, this can have very strange effects.

Without looking at the empirical data we can assume that we perhaps have a mean `Effort` in the thousands (not much of a project otherwise). The maximum `Effort` could very well end up in the millions (50--60 people working for ten years in a project), but let's assume it's not in the billions (70,000 people working for ten years in a project).

```{r}
# sample randomly from a log-normal() and check max value
max(rlnorm(1e5, 0, 4))
```

which seems OK. Let's have a look at a default prior, which we see commonly used,

```{r}
# sample randomly from a log-normal()
max(rlnorm(1e5, 0, 10))
```

which seems absurd. Hard to see a project with these many hours\ldots Such a project would need the earth's population ($8 \cdot 10 ^9$) to work on the project for almost 17,000 years, if we assume engineers spend 1,500 hours/year on the project: $2 \cdot 10^{17} / 8 \cdot 10^9 / 1.5 \cdot 10^3 =$ `r round(2e17 / 8e9 / 1.5e3,0)`.

Let's use a $\mathsf{Normal}(0,4)$ as a prior on the $\alpha$ parameter that we want to estimate. 

## Step 3: Calculating the posterior

Sample the model with empirical data. We sample using four independent chains, which then are used to indicate if we have reached a stationary posterior distribution.

```{r m0, message=FALSE, warning=FALSE, results='hide'}
m0 <- ulam(
  alist(
    Effort ~ poisson(lambda),
    log(lambda) <- alpha, # log link
    alpha ~ normal(0, 3)
  ), data = f, cores = 4, chains = 4, cmdstan = TRUE, log_lik = TRUE, iter = 5e3
)
```

Check diagnostics.

```{r}
precis(m0)
```

The diagnostics looks OK. $\widehat{R} < 1.01$ and the effective sample size (`n_eff`) is in the thousands, so all is well there. Let's also have a look at the trankplots for our estimated parameters.

```{r, echo=FALSE, warning=FALSE, message=FALSE, results='hide', fig.align='center'}
trankplot(m0)
traceplot(m0)
```

This is how healthy trank- or traceplots should look like, i.e., four chains mixing well after the initial phase. 

The estimated $\alpha$ was $\sim 8.5$. But remember, we used a log link so we need to get it back on the outcome scale, i.e., $\mathrm{exp}(8.5)=$ `r round(exp(precis(m0)[,1]), 0)`. That's the overall mean of `Effort`, no matter language used. 

Conducting posterior predictive checks when we only estimate a grand mean is really not sane so we'll leave this for now. Let's focus on adding adding our predictor `Language`.

We'll add `Language` as a varying intercept, i.e., each of the three languages get their own intercept.

```{r m1, message=FALSE, warning=FALSE, results='hide'}
m1 <- ulam(
    alist(
        Effort ~ poisson(lambda),
        log(lambda) <- a_lang[Language], # each lang its own intercept
        a_lang[Language] ~ normal(0, 3)
    ), data = f, cores = 4, chains = 4, cmdstan = TRUE, log_lik = TRUE
)
```

We've conducted prior predictive checks and checked diagnostics for the above model. Now it might be interesting to do a posterior predictive check, i.e., how well our model fits the data.

## Step 4: Sanity check of the posterior

```{r, message=FALSE, warning=FALSE, results='hide'}
postcheck(m1, window = 81)
```

The vertical axis is the outcome for each case in our dataset on the horizontal. The blue dots are the empirical data. The circles are the posterior mean. We see that the model has terribly fit in many cases (i.e., many of the blue dots are far away from the circles). Our priors are sane, we do know that, but given this large variation the $\mathsf{Poisson}$ might not be the right likelihood after all$\ldots$ Remember, the $\mathsf{Poisson}$ assumes that the mean and the variance is approximately equal. Let's check if this is true,

```{r}
var(f$Effort)
mean(f$Effort)
```

Far away. This indicates that the $\mathsf{Poisson}$ should not be used. We need to fall back on a mixture distribution (a mixture of $\mathsf{Gamma}$ and $\mathsf{Poisson}$) often called $\mathsf{Negative}$-$\mathsf{Binomial}$. Here we will be able to model the variance separately.

```{r, m2, message=FALSE, warning=FALSE, results='hide'}
m2 <- ulam(
    alist(
        Effort ~ dgampois(lambda, phi), # phi to model variance
        log(lambda) <- a_lang[Language],
        a_lang[Language] ~ dnorm(0, 3),
        phi ~ dexp(1) # prior on the variance component phi
    ), data = f, cores = 4, chains = 4, cmdstan = TRUE, log_lik = TRUE,
    control = list(adapt_delta = 0.95)
)
```

Let's do a posterior check again,

```{r}
postcheck(m2, window = 81)
```

The blue dots are the empirical data. The circles are the posterior mean, with 90% interval, and the '+' symbol mark the 90% predicted interval. In some cases we see clear outliers (e.g., Cases 44 and 81), which are outside the 90% predicted interval. This is not strange, after all, if we want a perfect model for our empirical data, why not simply use the data as-is? Well, the answer to that question is: We will inadvertently overfit, i.e., learn too much from the empirical data, which will lead to a model that will break down when facing new data.

## (Step 5: Model comparisons)

We now have a set of models $\mathbf{M}$, which we can compare using PSIS-LOO. The comparison will be *relative* and will not indicate if we've found an optimal, 'true', model. It will simply rank the models according to their relative out of sample prediction capabilities.

```{r loo, warning=FALSE, message=FALSE, fig.align='center'}
(loo_est <- compare(m0, m1, m2, func=LOO))
```

What we see here, not surprisingly, is that $\mathcal{M}_2$ is considered the best model. But how much better is that model compared to the model that comes on the second place, i.e., $\mathcal{M}_1$?

We can actually calculate a confidence interval for it if we want to. Using a $z$-score of 1.96 (i.e., 95%) we can use the relative difference in PSIS (dPSIS) and the difference in standard error (dSE) to calculate the interval,

```{r}
loo_est[2,3] + c(-1,1) * loo_est[2,4] * 1.96
```

which is clearly not crossing zero. In short, modeling the variance separately using the $\mathsf{Negative}$-$\mathsf{Binomial}$ paid off big time.

## Step 6: Compute stuff

If we plot our estimates we might visually appreciate the differences between the estimates better.

```{r}
plot(precis(m2, depth = 2, pars = "a_lang", prob = 0.95))
```

Circles indicate the estimated posterior mean, and the bars are 95% posterior intervals. 

We see that one language (Language 3) is clearly lower than the other languages. In short, using Language 3 means that a project uses less effort. Since Language 3 clearly does not 'touch' any of the other languages, one could claim that it's a 'significant' difference. But then there are many other things that could prove this to be false$\ldots$

# Appendix

## CmdStan

For the sampling we refrain from using [rstan](https://mc-stan.org/users/interfaces/rstan) and instead use [cmdstan](https://mc-stan.org/users/interfaces/cmdstan) through the <font style="font-family: serif">R</font> package [rethinking](https://github.com/rmcelreath/rethinking). Generally speaking, the community now prefer users to use `cmdstan` since it updates more frequently.

Install `cmdstanr` and `cmdstan` by,

```{r, eval=FALSE}
CORES = 8 # set to the number of available CPU cores
remotes::install_github("stan-dev/cmdstanr")
cmdstanr::install_cmdstan(cores = CORES)
# you can now run rethinking with cmdstan instead of rstan
```

For this execution we've used,

```{r}
cmdstanr::cmdstan_version()
```

## Environment

```{r}
print(sessionInfo(), locale = FALSE)
```
